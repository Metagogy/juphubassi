{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Load, Preprocess and split the Parts Of Speech tagged corpora from NLTK. Use a Pre Trained model as word embedding as GoogleNews-vectors. Build, Compile , Train and Evaluate the model with pre-trained model by layer as GRU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Description** :\n",
    "\n",
    "load POS tagged corpora from NLTK by using brown, treebank and conll2000. Import these three libraries from NLTK corpus\n",
    "\n",
    "Divide data into words ( X ) and tags ( Y ) using an empty list as  X and Y and store in it.\n",
    "\n",
    "Convert Text to integer by using text_to_sequences\n",
    "\n",
    "Truncate long sentences into fixed lengths as 100 \n",
    "\n",
    "Convert classes to binary form by using to_categorical\n",
    "\n",
    "Split data into training  and  testing sets ( test set as 0.15 )\n",
    "\n",
    "Split training data into training and validation sets ( Valid set as 0.15 )\n",
    "\n",
    "Use a pre-trained model as word embedding as google news vector. load word2vec using the following function present in the gensim library\n",
    "\n",
    "assign word vectors from word2vec model and each word in word2vec model is represented using a 300 dimensional vector\n",
    "\n",
    "create an empty embedding matrix and create a word to index dictionary mapping\n",
    "\n",
    "copy vectors from word2vec model to the words present in corpus\n",
    "\n",
    "Build the model by using Sequential API with adding layers as embedding with different dimensions as input_dim     =  VOCABULARY_SIZE,  output_dim    =  EMBEDDING_SIZE,  input_length  =  MAX_SEQ_LENGTH,   weights       = [embedding_weights], trainable     =  True  and add second layer as GRU with 64 neurons and return_sequences = True and add third layer as TimeDistributed layer with also dense layer as NUM_CLASSES and activation function as softmax ( Here Num classes = 13 )\n",
    "\n",
    "\n",
    "Compile the model with loss as Categorical_crossentropy , optimizer as adam and metrics as accuracy\n",
    "\n",
    "Fit or train the model with training sets , epochs = 5 , batch_size = 128 and validation sets\n",
    "\n",
    "Evaluate the model with testing sets as loss and accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Level**: Hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Format** : \n",
    "POS tagged corpora from NLTK and Word2Vec model from Google News vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Format** : \n",
    "Neural network model accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Input** : \n",
    "Load the dataset using libraries called brown, treebank , conll2000 and load word2vec using the following function present in the gensim library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Output** : \n",
    "Accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
